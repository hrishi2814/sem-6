## üìä What is a Confusion Matrix?

A **Confusion Matrix** is a table used to evaluate the performance of a **classification model**.  
It compares the **actual labels** with the **predicted labels** by the model.

---

### ‚úÖ Format of a Confusion Matrix (Binary Classification)

||**Predicted: Positive**|**Predicted: Negative**|
|---|---|---|
|**Actual: Positive**|True Positive (TP)|False Negative (FN)|
|**Actual: Negative**|False Positive (FP)|True Negative (TN)|

---

### üîç Meaning of Terms

- **True Positive (TP)**: Model correctly predicted Positive
    
- **True Negative (TN)**: Model correctly predicted Negative
    
- **False Positive (FP)**: Model predicted Positive, but it's actually Negative (Type I error)
    
- **False Negative (FN)**: Model predicted Negative, but it's actually Positive (Type II error)
    

---

## üß† Example

Let‚Äôs say we have a model detecting whether an email is **spam** or **not spam**.

|Email|Actual|Predicted|
|---|---|---|
|1|Spam|Spam|
|2|Spam|Not Spam|
|3|Not Spam|Spam|
|4|Not Spam|Not Spam|

From this:

- **TP** = 1 (Correctly predicted spam)
    
- **FN** = 1 (Spam predicted as not spam)
    
- **FP** = 1 (Not spam predicted as spam)
    
- **TN** = 1 (Correctly predicted not spam)
    

---

## üìê Performance Metrics from Confusion Matrix

You can calculate various metrics:

1. **Accuracy**: Overall correctness
    
    Accuracy=TP+TNTP+TN+FP+FN\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
2. **Precision**: How many predicted positives are actually correct
    
    Precision=TPTP+FP\text{Precision} = \frac{TP}{TP + FP}
3. **Recall (Sensitivity)**: How many actual positives were correctly predicted
    
    Recall=TPTP+FN\text{Recall} = \frac{TP}{TP + FN}
4. **F1 Score**: Harmonic mean of Precision and Recall
    
    F1=2√óPrecision‚ãÖRecallPrecision+Recall\text{F1} = 2 \times \frac{Precision \cdot Recall}{Precision + Recall}

---

## üéØ Why Use a Confusion Matrix?

- Gives a **clear picture** of model performance beyond just accuracy.
    
- Helps identify whether the model is making more **Type I** or **Type II** errors.
    
- Useful in **imbalanced datasets** (e.g., 95% of emails are not spam).
    

---

## üõ† Python Example

```python
from sklearn.metrics import confusion_matrix

y_true = [1, 1, 0, 0]  # actual: 1=spam, 0=not spam
y_pred = [1, 0, 1, 0]  # predicted

cm = confusion_matrix(y_true, y_pred)
print(cm)
```

Output:

```
[[1 1]
 [1 1]]
```

Which means:

- TP = 1, TN = 1, FP = 1, FN = 1
    

---

## üìå Summary

- **Confusion Matrix** is essential for evaluating classification models.
    
- It tells **what kinds of errors** your model is making.
    
- Helps compute performance metrics like **Precision**, **Recall**, and **F1-Score**.
    

Let me know if you want a visual chart or code that plots the confusion matrix using matplotlib or seaborn!